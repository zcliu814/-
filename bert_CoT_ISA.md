# 简单了解



[TOC]

## Reasoning Implicit Sentiment with Chain-of-Thought Prompting

## ACL 2023 | 利用思维链（CoT）推理隐式情感

### 论文 

https://arxiv.org/abs/2305.11255

### 代码

https://github.com/scofield7419/THOR-ISA







## 背景

情感分析（SENTIMENT ANALYSIS）: 检测输入文本对给定目标的情感极性

**情感词是否给定**分类：**显式**情感分析（Explicit SA，ESA），**隐式**情感分析（Implicit SA，ISA）

显示情感分析输入包含情感相关的表述词，主流（几乎都是找情感特征词才能预测）

隐式情感分析输入只包含中性的事实描述，没有显式的情感线索词，困难（对比人类总是能分析出文本的情感）



人类的ISA模式：

对于文本，人类会根据上下文，进一步获取更多信息，消除更多的不确定性

快去尝尝淄博的烧烤吧，人类会一步步得出这是积极的情感

常识：淄博地点，烧烤食物，

同时，这其实是一个步步递进式的推理过程。在此之中，我们往往会先确定所谈论的目标的具体方面，比如对于“淄博的烧烤”，其实是想去理解“淄博的烧烤”的“味道”。接着再对于“味道”这个方面，进一步根据蛛丝马迹思考用户的所持有的真正意图和观点，比如会有这么一个思考推理过程：由于用户在强烈推荐别人去尝尝“淄博的烧烤”，那么其“味道”是值得推荐的。最后根据这些所推理出来的丰富的中间上下文信息，便可轻易地且正确地确定最终的情感极性，积极。

ISA是一个步步推理的过程，要一步步的去揭示上下文，隐含信息

- 这个推理过程实际上完美地对应了现有的细粒度方面级别情感分析（Aspect-based Sentiment Analysis, ABSA）的定义，即先确定方面（Aspect），再挖掘意见（Opinion），最终得到情感极性（Polarity）。其中中间的 Aspect 与 Opinion 是隐式的，需要通过推理得到，才能构成完整的情感版图。
- 这个推理过程可以更确切地拆分为两种推理能力：一个是**常识推理能力**（即推断“淄博的烧烤”是什么），另一个是**多跳推理能力**（即推断方面，然后推断观点，再确定最终极性）。

> **细粒度方面级情感分析**：

传统情感分析：找对象对应的情感词即可

问题：错误识别对象和情感词，复杂情感，（引入知识或预训练解决）

没有情感词（大模型擅长）



### 具体方式

通过提示询问大模型，即prompt，但是应用思维链的方式

三步：

- 给定句子和对象，询问句子中提到了对象的哪个方面？
- 给定句子、对象、方面，询问句子针对该对象的该方面上发表了怎么样的观点？
- 给定句子、对象、方面、观点，询问句子在该对象上的情感倾向是什么？





两种方法来增强大模型的表现

1. self-consistency: 简单来说就是让大模型生成多次答案，然后投票
2. reasoning revising with supervision：这里指的是有监督微调，但是每一步怎么微调呢？作者这里是将每一步的问题+答案+what is the sentiment polarity towards t?作为输入，然后在模型的输出和真实值上计算损失。

可以看到去掉reasoning revising with supervision是zero-shot，不去掉就是全监督了。

zero-shot：使模型在未经过显式训练的类别或任务上进行泛化并进行预测



## 隐式情感三跳推理框架

**预训练语言模型：**

**常识推理**：

**多跳推理**：模型可以通过多个步骤的推理来逐渐获得更全面的信息，以便解决更复杂的问题。

幸运的是，近来大规模预训练语言模型（Large Language Models, LLMs），诸如 ChatGPT、Vicuna、LLaMA、GPT4 等，取得了巨大的成功，这为我们的问题提供了一个很好的解决方案：

- 一方面，由于经历过超大规模的预训练，LLMs 被广泛证明学习到了非常丰富的世界知识，在常识理解方面展现出了非凡的能力，因此可以轻松胜任常识性推理。
- 另一方面，最新的思维链（CoT）理念揭示了 LLMs 的多跳推理的巨大潜力，在这种推理中，合理地提示 LLM 便可进行出色的链式推理。

（这个也是堆叠了东西）：提示词prompt工程（可以尝试一下）

基于一个基座 LLM，我们设计了三种 Prompt 来进行三个推理步骤，由易到难，分别推断出：

1）给定目标的细粒度方面（Aspect），

2）对该方面的潜在观点（Opinion），

 3）最终的极性（Polarity）。



### 思维链提示模板

输入的句子x

待分析目标t

极性y

中间的Aspect为a

潜在的Opinion为o

**三跳 Prompt 模板**

**argmax：**

**第一步：**我们首先询问 LLM 句子中涉及到关于哪一种方面 a，使用以下模板：

C1 是第一跳提示的上下文。这一步可以表示为 A=argmax p(a|X, t)，其中 A 是明确提到方面 a 的输出文本。

**第二步：**现在基于 X、t 和 a，我们要求 LLM 详细回答关于提到方面 a 的潜在观点 o 是什么：

C2 是第二跳提示的上下文，我们将 C1 和 A 连接在一起。这一步可以写成 O=argmax p(o|X, t, a)，其中 O 是包含可能观点表达 o 的答案文本。

**第三步：**在完整的情感框架（X、t、a 和 o）作为上下文的基础上，我们最终要求LLM 推断出极性 t 的最终答案：

C3 是第三跳提示的上下文。我们将这一步表示为 y=argmax p(y|X, t, a, o)。

**增强推理的自一致性**

我们进一步利用自一致性机制来巩固推理的正确性。具体来说，在每个三步推理的过程中，我们设置 LLM 解码器生成多个答案，其中每个答案可能会对方面 a、观点 o 以及极性 y 给出不同的预测。在每一步中，我们保留那些推断得到的 a、o 或 y 具有高投票一致性的答案。我们选择置信度最高的答案作为下一步的上下文。

**通过监督进行推理修正**

当存在带有标注的训练集时（即有监督的微调设置），我们还可以对 THOR 进行微调。我们设计了一种推理修正方法。具体地，在每个步骤中，我们通过连接以下内容构建一个提示：1）初始上下文，2）这一步的推理答案文本和 3）最终的问题，然后将其输入 LLM 以预测情感标签，而不是继续进行下一步推理。例如，在第一步的末尾，我们可以组合一个提示：[C1, A，‘对 t 的情感极性是什么？’]。在使用 Gold 标签进行监督时，LLM 将被指导生成更多正确的中间推理，这对于最终的预测是有利的。



## 数据集

SemEval14 Laptop 和 Restaurant ，所有实例被划分为ESA和ISA

使用了 Flan-T5 作为我们的主干 LLM；以及 GPT3 和 ChatGPT 进行测试





理解（要找最新的思路论文，和大模型相互结合。尽量解决那些新的领域，没人涉及过的）

关键词：提示prompt,大模型，思维链，推理，情感分析等领域(情感分析可能不是很好的方向，gpt已经解决大部分)

思维链prompt



## 总结

通过步步递进式、由易到难的渐进推理诱导 LLM 得到丰富的中间上下文信息帮助推断情感极性

问题：11b的参数量相对于提升来说消耗太大了

思路：结合cot 用公开的大模型尝试知识推理

#### 参考文献

[ACL 2023 | 利用思维链（CoT）推理隐式情感，狂涨50%_分析_淄博_烧烤 (sohu.com)](https://www.sohu.com/a/686298571_121119001)

[ACL'23 | 一种思维链引导的情感分析方法 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/638660670)